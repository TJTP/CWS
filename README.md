# [NJUAI] NLP课程作业一: 中文分词
## 文件及运行方式说明
* `MyTrain.py`是训练模型步骤的实现, 可以直接运行该文件, 得到HMM方法和BIMM方法需要的模型 
  * 不同的模型在文件名中由训练时间区分, 保存至`./Models/`文件夹.
* `MyHMM.py`包含隐马尔可夫方法在应用步骤的实现, 以及一些改进双向匹配方法需要的函数. 可以直接运行该文件, 得到测试集的分词结果 
  * 需要给定模型的编号
  * 结果保存到`./Results/`文件下, 由文件名中的运行时间区分
* `MyBIMM.py`包含双向匹配方法在应用步骤的实现, 可以直接运行该文件, 得到测试集的分词结果 
  * 需要给定模型的编号
  * 结果保存到`./Results/`文件下, 由文件名中的`"bi"`以及运行时间区分
  * **本次实验的结果由该方法生成**
* `MyUtils`包含了多个文件中需要的函数以及常量
## 实现方法
### 原始实现
&emsp;&emsp;在最初的实验中, 我采用了HMM的方法, 但是发现在该训练集和测试集上, 效果并不是很好, $f1$值在$0.81$左右. 于是我又改用了简单的双向匹配方法, $f1$值来到了$0.85$左右. 双向匹配, 其实就在前向匹配和逆向匹配的结果中, 选择最优的一个作为最终的分词结果. 所以接下来, 我就针对前向匹配和逆向匹配中的规则, 双向匹配时选择最优的依据这两个点进行了改进.  

### 匹配规则的改进
&emsp;&emsp;首先来看匹配规则上的改进, 我注意到, 在训练集和开发集中, 阿拉伯数字或者中文的数字与某些特定的字符分割在一起, 因此我建立了如下两个列表:

        #中文数字, 匹配时合在一起
        quanChars = ['零', '一', '二', '三', '四', '五', '六', '七', '八', '九', \
                        '十', '廿', '百','千', '万', '亿']
        #这类字符在他们前面出现阿拉伯或中文数字时, 合在一起
        dateChars = ['时', '日', '天', '月', '年', '后', '件', '个']
在匹配时, 如果发现有出现上面列表中的字符, 就去检查, 看能否合并在一起. 对于阿拉伯数字, 我还另外建立了一个列表:

        #当阿拉伯数字中间出现这些字符时, 连在一起
        digiChars = ['.', ':']
当阿拉伯数字中出现这些字符时, 意味着这是一个小数或者一个时刻, 应当合在一起. 针对英文单词, 我也设定了规则进行检查, 保证英文单词合在一起.   

&emsp;&emsp;而对于未在字典中出现的字符, 在原始的双向匹配方法中, 直接将它们作为单个字符加入到划分结果中, 我在改进时利用HMM方法来帮助判断这些字符是否可能组成一个新的词语. 以下面这句为例:

        《经济日报》产经新闻部主任崔书文：负面清单管理制度的推出和修订，提高了开放度和透明度。
对于姓名"崔书文", 在训练集中没有这个词语, 所以在前向匹配和逆向匹配中, 都会把它分成三个单独的字. 而用HMM对这句话进行划分, 发现将这三个字合并在了一起, 所以在遇到训练集中未出现的词语时, 我将整句话用Viterbi算法标注好, 然后找到这个字最近的一个划分点, 当然可能最终也是把它当成一个单字来划分. 逆向匹配中也使用了这样的规则, 这部分的功能调用了实现在`MyHMM.py`中的`seg_substr_f()`和`seg_substr_b()`的函数. 

&emsp;&emsp;除了对未在训练集中记录的字符采用HMM进行划分, 对于存在于训练集中的单字, 当它的概率太小, 即它在训练集中出现次数非常少时, 我们有理由认为它通常不会作为一个单字, 那么这个时候也可以调用HMM来进行划分. 经过观察, 发现在概率取对数后, 常见的一些单字, 比如"与", "和"等, 其值都在$-10.0$以上, 因此将阈值设置为$-10.0$, 这个值存储于变量`probThreshold`中. 如果单字的概率取对数后低于这个阈值, 我们就对它调用HMM进行划分. 在上面那句话中, 逆向匹配到"崔书文"时, 会发现"文"存在于训练集中, 但是"文"的概率对数小于我们设定的阈值, 于是就对它采用HMM划分, 得到"崔书文"这个划分. 

### 选择依据的改进
&emsp;&emsp;通常来说, 逆向匹配的结果要优于正向匹配, 但是我们仍然需要一些判断依据来帮助我们决定最终采取哪一个划分结果. 在我的原始实现中, 我将惩罚分设定为单字个数加上划分词的总数, 效果还算不错, 不过仍然有提升的地方. 

&emsp;&emsp;我在上述两条惩罚分的构成之外, 还定义了两种指标, 一个是句子内最大连续单字长度 (仅包含汉字) , 比如`['一', '二', '数字', '三', '四']`的最大连续单字长度就是2. 另一个指标是概率对数的和, 其绝对值 (因为取对数后是负数) 越大, 意味着分到了更多小概率词, 说明划分得不够好. 由于概率对数的和的绝对值比较大, 为了不抵消掉其他几个指标的作用, 我对它进行了一个乘上$0.1$的处理. 对于这四个指标, 我还对它们设定了权重, 来调整它们在选择中发挥的作用. 这部分的代码如下: 

        weights = [10, 34, 42, 42]
        wcWeight = weights[0] /sum(weights) #分词数量的权重
        scWeight = weights[1] /sum(weights) #单字数量的权重
        msWeight = weights[2] /sum(weights) #最大连续单字数量的权重
        prWeight = weights[3] /sum(weights) #概率对数的和的权重

        normalizer = 0.1 #概率对数的和的归一化因子
        #正向匹配结果的惩罚分数
        fPenScore = wcWeight * fWordCnt + scWeight * fSingleCnt + \
                        msWeight * fMaxSingle + prWeight * (-fProbSum) * normalizer
        #逆向匹配结果的惩罚分数
        bPenScore = wcWeight * bWordCnt + scWeight * bSingleCnt + \ 
                        msWeight * bMaxSingle + prWeight * (-bProbSum) * normalizer   

## 实验总结 
&emsp;&emsp;双向匹配方法和隐马尔可夫方法在分词上各有各的优势, 前者对于存在于词表中的词语能有良好的划分, 而后者能利用统计的规律对一些不在词表中的字符进行划分. 因此把两者结合到一起发挥各自的优点就是很自然的想法.   

&emsp;&emsp;同时也要注意到, 参数的调整, 评测指标或是特征的选择对于分词结果也有很大的影响. 参数的调整需要通过多次实验来逐步调整, 比如在确定正向匹配和逆向匹配的惩罚分时, 各部分的权重对结果有着巨大影响. 在实验中发现, 一个较合理的参数组合能将$f1$值提升到$0.89$以上, 而一个不太合理的参数组合可能只有$0.87$甚至更低, 最终的结果会有两个百分点的差距. 此外还需要观察训练集的特点来调整, 比如单字的概率对数阈值`probThreshold`的确定就是基于对训练集构造出的词表的观察. 